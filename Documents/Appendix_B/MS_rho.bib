
@article{altoeEnhancingStatisticalInference2020,
  title = {Enhancing {{Statistical Inference}} in {{Psychological Research}} via {{Prospective}} and {{Retrospective Design Analysis}}},
  author = {Alto{\`e}, Gianmarco and Bertoldo, Giulia and Zandonella Callegher, Claudio and Toffalini, Enrico and Calcagn{\`i}, Antonio and Finos, Livio and Pastore, Massimiliano},
  year = {2020},
  volume = {10},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2019.02893},
  abstract = {In the past two decades, psychological science has experienced an unprecedented replicability crisis, which has uncovered several issues. Among others, the use and misuse of statistical inference plays a key role in this crisis. Indeed, statistical inference is too often viewed as an isolated procedure limited to the analysis of data that have already been collected. Instead, statistical reasoning is necessary both at the planning stage and when interpreting the results of a research project. Based on these considerations, we build on and further develop an idea proposed by Gelman and Carlin (2014) termed ``prospective and retrospective design analysis.'' Rather than focusing only on the statistical significance of a result and on the classical control of type I and type II errors, a comprehensive design analysis involves reasoning about what can be considered a plausible effect size. Furthermore, it introduces two relevant inferential risks: the exaggeration ratio or Type M error (i.e., the predictable average overestimation of an effect that emerges as statistically significant) and the sign error or Type S error (i.e., the risk that a statistically significant effect is estimated in the wrong direction). Another important aspect of design analysis is that it can be usefully carried out both in the planning phase of a study and for the evaluation of studies that have already been conducted, thus increasing researchers' awareness during all phases of a research project. To illustrate the benefits of a design analysis to the widest possible audience, we use a familiar example in psychology where the researcher is interested in analyzing the differences between two independent groups considering Cohen's d as an effect size measure. We examine the case in which the plausible effect size is formalized as a single value, and we propose a method in which uncertainty concerning the magnitude of the effect is formalized via probability distributions. Through several examples and an application to a real case study, we show that, even though a design analysis requires significant effort, it has the potential to contribute to planning more robust and replicable studies. Finally, future developments in the Bayesian framework are discussed.},
  copyright = {All rights reserved},
  file = {/Users/claudio/MEGA/Zotero/Alto√® et al_2020_Enhancing Statistical Inference in Psychological Research via Prospective and.pdf},
  journal = {Frontiers in Psychology},
  language = {en}
}

@article{buttonPowerFailureWhy2013,
  title = {Power Failure: Why Small Sample Size Undermines the Reliability of Neuroscience},
  author = {Button, K.S. and Ioannidis, J.P.A. and Mokrysz, C. and Nosek, B.A. and Flint, J. and Robinson, E.S.J. and Munaf{\`o}, M.R.},
  year = {2013},
  volume = {14},
  pages = {365--376},
  doi = {10.1038/nrn3475},
  journal = {Nature Reviews Neuroscience},
  language = {English},
  number = {5}
}

@article{camererEvaluatingReplicabilityLaboratory2016,
  title = {Evaluating Replicability of Laboratory Experiments in Economics},
  author = {Camerer, Colin F. and Dreber, Anna and Forsell, Eskil and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Almenberg, Johan and Altmejd, Adam and Chan, Taizan and Heikensten, Emma and Holzmeister, Felix and Imai, Taisuke and Isaksson, Siri and Nave, Gideon and Pfeiffer, Thomas and Razen, Michael and Wu, Hang},
  year = {2016},
  volume = {351},
  pages = {1433--1436},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.aaf0918},
  abstract = {Experimental economists have joined the reproducibility discussion by replicating selected published experiments from two top-tier journals in economics. Camerer et al. found that two-thirds of the 18 studies examined yielded replicable estimates of effect size and direction. This proportion is somewhat lower than unaffiliated experts were willing to bet in an associated prediction market, but roughly in line with expectations from sample sizes and P values.Science, this issue p. 1433The replicability of some scientific findings has recently been called into question. To contribute data about replicability in economics, we replicated 18 studies published in the American Economic Review and the Quarterly Journal of Economics between 2011 and 2014. All of these replications followed predefined analysis plans that were made publicly available beforehand, and they all have a statistical power of at least 90\% to detect the original effect size at the 5\% significance level. We found a significant effect in the same direction as in the original study for 11 replications (61\%); on average, the replicated effect size is 66\% of the original. The replicability rate varies between 67\% and 78\% for four additional replicability indicators, including a prediction market measure of peer beliefs.},
  journal = {Science},
  number = {6280}
}

@article{camererEvaluatingReplicabilitySocial2018,
  ids = {camerer2018a},
  title = {Evaluating the Replicability of Social Science Experiments in {{Nature}} and {{Science}} between 2010 and 2015},
  author = {Camerer, Colin F. and Dreber, Anna and Holzmeister, Felix and Ho, Teck-Hua and Huber, J{\"u}rgen and Johannesson, Magnus and Kirchler, Michael and Nave, Gideon and Nosek, Brian A. and Pfeiffer, Thomas and Altmejd, Adam and Buttrick, Nick and Chan, Taizan and Chen, Yiling and Forsell, Eskil and Gampa, Anup and Heikensten, Emma and Hummer, Lily and Imai, Taisuke and Isaksson, Siri and Manfredi, Dylan and Rose, Julia and Wagenmakers, Eric-Jan and Wu, Hang},
  year = {2018},
  volume = {2},
  pages = {637--644},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0399-z},
  file = {/Users/claudio/MEGA/Zotero/Camerer et al_2018_Evaluating the replicability of social science experiments in Nature and.pdf},
  journal = {Nature Human Behaviour},
  keywords = {Letto,Replicability},
  language = {en},
  number = {9}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  year = {1988},
  publisher = {{Lawrence Erlbaum Associates}},
  file = {/Users/claudio/MEGA/Zotero/Cohen_1988_Statistical power analysis for the behavioral sciences.pdf},
  isbn = {978-0-8058-0283-2},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis},
  language = {en},
  lccn = {HA29 .C66 1988}
}

@article{ebersoleManyLabsEvaluating2016,
  title = {Many {{Labs}} 3: {{Evaluating}} Participant Pool Quality across the Academic Semester via Replication},
  author = {Ebersole, Charles R. and Atherton, Olivia E. and Belanger, Aimee L. and Skulborstad, Hayley M. and Allen, Jill M. and Banks, Jonathan B. and Baranski, Erica and Bernstein, Michael J. and Bonfiglio, Diane B.V. and Boucher, Leanne and Brown, Elizabeth R. and Budiman, Nancy I. and Cairo, Athena H. and Capaldi, Colin A. and Chartier, Christopher R. and Chung, Joanne M. and Cicero, David C. and Coleman, Jennifer A. and Conway, John G. and Davis, William E. and Devos, Thierry and Fletcher, Melody M. and German, Komi and Grahe, Jon E. and Hermann, Anthony D. and Hicks, Joshua A. and Honeycutt, Nathan and Humphrey, Brandon and Janus, Matthew and Johnson, David J. and {Joy-Gaba}, Jennifer A. and Juzeler, Hannah and Keres, Ashley and Kinney, Diana and Kirshenbaum, Jacqeline and Klein, Richard A. and Lucas, Richard E. and Lustgraaf, Christopher J.N. and Martin, Daniel and Menon, Madhavi and Metzger, Mitchell and Moloney, Jaclyn M. and Morse, Patrick J. and Prislin, Radmila and Razza, Timothy and Re, Daniel E. and Rule, Nicholas O. and Sacco, Donald F. and Sauerberger, Kyle and Shrider, Emily and Shultz, Megan and Siemsen, Courtney and Sobocko, Karin and Sternglanz], R. [Weylin and Summerville, Amy and Tskhay, Konstantin O. and Allen], Zack [van and Vaughn, Leigh Ann and Walker, Ryan J. and Weinberg, Ashley and Wilson, John Paul and Wirth, James H. and Wortman, Jessica and Nosek, Brian A.},
  year = {2016},
  volume = {67},
  pages = {68--82},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2015.10.012},
  abstract = {The university participant pool is a key resource for behavioral research, and data quality is believed to vary over the course of the academic semester. This crowdsourced project examined time of semester variation in 10 known effects, 10 individual differences, and 3 data quality indicators over the course of the academic semester in 20 participant pools (N=2696) and with an online sample (N=737). Weak time of semester effects were observed on data quality indicators, participant sex, and a few individual differences\textemdash conscientiousness, mood, and stress. However, there was little evidence for time of semester qualifying experimental or correlational effects. The generality of this evidence is unknown because only a subset of the tested effects demonstrated evidence for the original result in the whole sample. Mean characteristics of pool samples change slightly during the semester, but these data suggest that those changes are mostly irrelevant for detecting effects.},
  journal = {Journal of Experimental Social Psychology},
  keywords = {Cognitive psychology,Individual differences,Participant pool,Replication,Sampling effects,Situational effects,Social psychology}
}

@article{eisenbergerDoesRejectionHurt2003,
  title = {Does Rejection Hurt? {{An fMRI}} Study of Social Exclusion},
  author = {Eisenberger, Naomi I. and Lieberman, Matthew D. and Williams, Kipling D.},
  year = {2003},
  volume = {302},
  pages = {290--292},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075},
  doi = {10.1126/science.1089134},
  abstract = {A neuroimaging study examined the neural correlates of social exclusion and tested the hypothesis that the brain bases of social pain are similar to those of physical pain. Participants were scanned while playing a virtual ball-tossing game in which they were ultimately excluded. Paralleling results from physical pain studies, the anterior cingulate cortex (ACC) was more active during exclusion than during inclusion and correlated positively with self-reported distress. Right ventral prefrontal cortex (RVPFC) was active during exclusion and correlated negatively with self-reported distress. ACC changes mediated the RVPFC-distress correlation, suggesting that RVPFC regulates the distress of social exclusion by disrupting ACC activity.},
  journal = {Science},
  number = {5643}
}

@book{ellisEssentialGuideEffect2010,
  title = {The {{Essential Guide}} to {{Effect Sizes}}},
  author = {Ellis, Paul D},
  year = {2010},
  publisher = {{Cambridge University Press}},
  file = {/Users/claudio/MEGA/Zotero/Ellis_2010_The Essential Guide to Effect Sizes.pdf},
  keywords = {Letto Parzialmente},
  language = {en}
}

@article{fisherFrequencyDistributionValues1915,
  title = {Frequency {{Distribution}} of the {{Values}} of the {{Correlation Coefficient}} in {{Samples}} from an {{Indefinitely Large Population}}},
  author = {Fisher, R. A.},
  year = {1915},
  volume = {10},
  pages = {507},
  issn = {00063444},
  doi = {10.2307/2331838},
  file = {/Users/claudio/MEGA/Zotero/Fisher_1915_Frequency Distribution of the Values of the Correlation Coefficient in Samples.pdf},
  journal = {Biometrika},
  language = {en},
  number = {4}
}

@misc{fordAssessingTypeType2018,
  title = {Assessing {{Type S}} and {{Type M Errors}}},
  author = {Ford, Clay},
  year = {2018},
  file = {/Users/claudio/Zotero/storage/CU657DHD/assessing-type-s-and-type-m-errors.html},
  howpublished = {https://data.library.virginia.edu/assessing-type-s-and-type-m-errors/},
  journal = {University of Virginia Library}
}

@misc{gelmanEmbracingVariationAccepting2019,
  title = {Embracing {{Variation}} and {{Accepting Uncertainty}}: {{Implications}} for {{Science}} and {{Metascience}}},
  author = {Gelman, Andrew},
  year = {2019},
  abstract = {Full video of Andrew Gelman's presentation "Embracing Variation and Accepting Uncertainty: Implications for Science and Metascience" at the Metascience 2019 Symposium at Stanford.},
  file = {/Users/claudio/Zotero/storage/3E89279A/andrew-gelman.html},
  language = {en-US},
  type = {Video}
}

@article{gelmanFailureNullHypothesis2018,
  title = {The {{Failure}} of {{Null Hypothesis Significance Testing When Studying Incremental Changes}}, and {{What}} to {{Do About It}}},
  author = {Gelman, Andrew},
  year = {2018},
  volume = {44},
  pages = {16--23},
  issn = {0146-1672, 1552-7433},
  doi = {10.1177/0146167217729162},
  abstract = {A standard mode of inference in social and behavioral science is to establish stylized facts using statistical significance in quantitative studies. However, in a world in which measurements are noisy and effects are small, this will not work: selection on statistical significance leads to effect sizes which are overestimated and often in the wrong direction. After a brief discussion of two examples, one in economics and one in social psychology, we consider the procedural solution of open post-publication review, the design solution of devoting more effort to accurate measurements and within-person comparisons, and the statistical analysis solution of multilevel modeling and reporting all results rather than selection on significance. We argue that the current replication crisis in science arises in part from the ill effects of null hypothesis significance testing being used to study small effects with noisy data. In such settings, apparent success comes easy but truly replicable results require a more serious connection between theory, measurement, and data.},
  file = {/Users/claudio/MEGA/Zotero/Gelman_2018_The Failure of Null Hypothesis Significance Testing When Studying Incremental.pdf},
  journal = {Personality and Social Psychology Bulletin},
  keywords = {Bayesian,Letto,NHST,Replicability,Research Practice},
  language = {en},
  number = {1}
}

@article{gelmanPowerCalculationsAssessing2014,
  title = {Beyond {{Power Calculations}}: {{Assessing Type S}} ({{Sign}}) and {{Type M}} ({{Magnitude}}) {{Errors}}},
  shorttitle = {Beyond {{Power Calculations}}},
  author = {Gelman, Andrew and Carlin, John},
  year = {2014},
  volume = {9},
  pages = {641--651},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614551642},
  abstract = {Statistical power analysis provides the conventional approach to assess error rates when designing a research study. However, power analysis is flawed in that a narrow emphasis on statistical significance is placed as the primary focus of study design. In noisy, small-sample settings, statistically significant results can often be misleading. To help researchers address this problem in the context of their own studies, we recommend design calculations in which (a) the probability of an estimate being in the wrong direction (Type S [sign] error) and (b) the factor by which the magnitude of an effect might be overestimated (Type M [magnitude] error or exaggeration ratio) are estimated. We illustrate with examples from recent published research and discuss the largest challenge in a design calculation: coming up with reasonable estimates of plausible effect sizes based on external information.},
  file = {/Users/claudio/MEGA/Zotero/Gelman_Carlin_2014_Beyond Power Calculations.pdf},
  journal = {Perspectives on Psychological Science},
  keywords = {Effect Size,Letto,Power Analysis,Type S and Type M errror},
  language = {en},
  number = {6}
}

@unpublished{gelmanRetrospectiveDesignAnalysis2013,
  title = {Retrospective Design Analysis Using External Information},
  author = {Gelman, Andrew and Carlin, John},
  year = {2013},
  file = {/Users/claudio/Zotero/storage/9K97KB34/retropower5.pdf},
  type = {Unpublished}
}

@article{gelmanStatisticalCrisisScience2014,
  title = {The Statistical Crisis in Science},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2014},
  volume = {102},
  pages = {460--466},
  publisher = {{Sigma Xi, The Scientific Research Society}},
  doi = {10.1511/2014.111.460},
  journal = {American scientist},
  number = {6}
}

@article{gelmanTypeErrorMight2017,
  title = {Type {{M Error Might Explain Weisburd}}'s {{Paradox}}},
  author = {Gelman, Andrew and Skardhamar, Torbj\o rn and Aaltonen, Mikko},
  year = {2017},
  issn = {0748-4518, 1573-7799},
  doi = {10.1007/s10940-017-9374-5},
  abstract = {Objectives Simple calculations seem to show that larger studies should have higher statistical power, but empirical meta-analyses of published work in criminology have found zero or weak correlations between sample size and estimated statistical power. This is ``Weisburd's paradox'' and has been attributed by Weisburd et al. (in Crime Justice 17:337\textendash 379, 1993) to a difficulty in maintaining quality control as studies get larger, and attributed by Nelson et al. (in J Exp Criminol 11:141\textendash 163, 2015) to a negative correlation between sample sizes and the underlying sizes of the effects being measured. We argue against the necessity of both these explanations, instead suggesting that the apparent Weisburd paradox might be explainable as an artifact of systematic overestimation inherent in post-hoc power calculations, a bias that is large with small N.
Methods We discuss Weisburd's paradox in light of the concepts of type S and type M errors, and re-examine the publications used in previous studies of the so-called paradox.
Results We suggest that the apparent Weisburd paradox might be explainable as an artifact of systematic overestimation inherent in post-hoc power calculations, a bias that is large with small N.
Conclusions Speaking more generally, we recommend abandoning the use of statistical power as a measure of the strength of a study, because implicit in the definition of power is the bad idea of statistical significance as a research goal.},
  file = {/Users/claudio/MEGA/Zotero/Gelman et al_2017_Type M Error Might Explain Weisburd‚Äôs Paradox.pdf},
  journal = {Journal of Quantitative Criminology},
  language = {en}
}

@article{gelmanTypeErrorRates2000,
  title = {Type {{S}} Error Rates for Classical and {{Bayesian}} Single and Multiple Comparison Procedures},
  author = {Gelman, Andrew and Tuerlinckx, Francis},
  year = {2000},
  volume = {15},
  pages = {373--390},
  issn = {09434062},
  doi = {10.1007/s001800000040},
  abstract = {In classical statistics, the signi cance of comparisons (e.g., 1 ? 2) is calibrated using the Type 1 error rate, relying on the assumption that the true di erence is zero, which makes no sense in many applications. We set up a more relevant framework in which a true comparison can be positive or negative, and, based on the data, you can state \textbackslash{} 1 {$>$} 2 with con dence," \textbackslash{} 2 {$>$} 1 with con dence," or \textbackslash no claim with con dence." We focus on the Type S (for sign) error, which occurs when you claim \textbackslash{} 1 {$>$} 2 with con dence" when 2 {$>$} 1 (or vice-versa). We compute the Type S error rates for classical and Bayesian con dence statements and nd that classical Type S error rates can be extremely high (up to 50\%). Bayesian con dence statements are conservative, in the sense that claims based on 95\% posterior intervals have Type S error rates between 0 and 2.5\%. For multiple comparison situations, the conclusions are similar.},
  file = {/Users/claudio/MEGA/Zotero/Gelman_Tuerlinckx_2000_Type S error rates for classical and Bayesian single and multiple comparison.pdf},
  journal = {Computational Statistics},
  language = {en},
  number = {3}
}

@inbook{gigerenzerNullRitualWhat2004,
  title = {The {{Null Ritual}}: {{What You Always Wanted}} to {{Know About Significance Testing}} but {{Were Afraid}} to {{Ask}}},
  shorttitle = {The {{Null Ritual}}},
  booktitle = {The {{SAGE Handbook}} of {{Quantitative Methodology}} for the {{Social Sciences}}},
  author = {Gigerenzer, Gerd and Krauss, Stefan and Vitouch, Oliver},
  year = {2004},
  pages = {392--409},
  publisher = {{SAGE Publications, Inc.}},
  address = {{2455 Teller Road,~Thousand Oaks~California~91320~United States of America}},
  doi = {10.4135/9781412986311.n21},
  collaborator = {Kaplan, David},
  file = {/Users/claudio/MEGA/Zotero/Gigerenzer et al_2004_The Null Ritual.pdf},
  isbn = {978-0-7619-2359-6 978-1-4129-8631-1},
  language = {en}
}

@article{ioannidisEmergenceLargeTreatment2013,
  title = {Emergence of {{Large Treatment Effects From Small Trials}}\textemdash{{Reply}}},
  author = {Ioannidis, John P. A. and Pereira, Tiago V. and Horwitz, Ralph I.},
  year = {2013},
  volume = {309},
  pages = {768--769},
  issn = {0098-7484},
  doi = {10.1001/jama.2012.208831},
  abstract = {In Reply: We agree with Drs Batterham and Hopkins that small studies are not necessarily inherently flawed. However, probabilistically speaking, even in the absence of biases, small-sized trials are more prone to provide overestimates (or underestimates) compared with larger trials.Thus, evidence from scattered small studies is easier to distort than evidence from large trials because analyses with the most impressive results are more likely to be published compared with studies showing underestimated treatment effects.},
  journal = {JAMA},
  number = {8}
}

@article{ioannidisWhyMostDiscovered2008,
  title = {Why {{Most Discovered True Associations Are Inflated}}:},
  shorttitle = {Why {{Most Discovered True Associations Are Inflated}}},
  author = {Ioannidis, John P. A.},
  year = {2008},
  volume = {19},
  pages = {640--648},
  issn = {1044-3983},
  doi = {10.1097/EDE.0b013e31818131e7},
  abstract = {Newly discovered true (non-null) associations often have inflated effects compared with the true effect sizes. I discuss here the main reasons for this inflation. First, theoretical considerations prove that when true discovery is claimed based on crossing a threshold of statistical significance and the discovery study is underpowered, the observed effects are expected to be inflated. This has been demonstrated in various fields ranging from early stopped clinical trials to genome-wide associations. Second, flexible analyses coupled with selective reporting may inflate the published discovered effects. The vibration ratio (the ratio of the largest vs. smallest effect on the same association approached with different analytic choices) can be very large. Third, effects may be inflated at the stage of interpretation due to diverse conflicts of interest. Discovered effects are not always inflated, and under some circumstances may be deflated\textemdash for example, in the setting of late discovery of associations in sequentially accumulated overpowered evidence, in some types of misclassification from measurement error, and in conflicts causing reverse biases. Finally, I discuss potential approaches to this problem. These include being cautious about newly discovered effect sizes, considering some rational down-adjustment, using analytical methods that correct for the anticipated inflation, ignoring the magnitude of the effect (if not necessary), conducting large studies in the discovery phase, using strict protocols for analyses, pursuing complete and transparent reporting of all results, placing emphasis on replication, and being fair with interpretation of results.},
  file = {/Users/claudio/MEGA/Zotero/Ioannidis_2008_Why Most Discovered True Associations Are Inflated.pdf},
  journal = {Epidemiology},
  language = {en},
  number = {5}
}

@article{kleinInvestigatingVariationReplicability2014,
  title = {Investigating {{Variation}} in {{Replicability}}},
  author = {Klein, Richard A. and Ratliff, Kate A. and Vianello, Michelangelo and Adams, Reginald B. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Bernstein, Michael J. and Bocian, Konrad and Brandt, Mark J. and Brooks, Beach and Brumbaugh, Claudia Chloe and Cemalcilar, Zeynep and Chandler, Jesse and Cheong, Winnee and Davis, William E. and Devos, Thierry and Eisner, Matthew and Frankowska, Natalia and Furrow, David and Galliani, Elisa Maria and Hasselman, Fred and Hicks, Joshua A. and Hovermale, James F. and Hunt, S. Jane and Huntsinger, Jeffrey R. and IJzerman, Hans and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Barry Kappes, Heather and Krueger, Lacy E. and Kurtz, Jaime and Levitan, Carmel A. and Mallett, Robyn K. and Morris, Wendy L. and Nelson, Anthony J. and Nier, Jason A. and Packard, Grant and Pilati, Ronaldo and Rutchick, Abraham M. and Schmidt, Kathleen and Skorinko, Jeanine L. and Smith, Robert and Steiner, Troy G. and Storbeck, Justin and Van Swol, Lyn M. and Thompson, Donna and {van `t Veer}, A. E. and Ann Vaughn, Leigh and Vranka, Marek and Wichman, Aaron L. and Woodzicka, Julie A. and Nosek, Brian A.},
  year = {2014},
  volume = {45},
  pages = {142--152},
  issn = {1864-9335},
  doi = {10.1027/1864-9335/a000178},
  abstract = {Although replication is a central tenet of science, direct replications are rare in psychology. This research tested variation in the replicability of 13 classic and contemporary effects across 36 independent samples totaling 6,344 participants. In the aggregate, 10 effects replicated consistently. One effect \textendash{} imagined contact reducing prejudice \textendash{} showed weak support for replicability. And two effects \textendash{} flag priming influencing conservatism and currency priming influencing system justification \textendash{} did not replicate. We compared whether the conditions such as lab versus online or US versus international sample predicted effect magnitudes. By and large they did not. The results of this small sample of effects suggest that replicability is more dependent on the effect itself than on the sample and setting used to investigate the effect.},
  file = {/Users/claudio/MEGA/Zotero/Klein et al_2014_Investigating Variation in Replicability.pdf},
  journal = {Social Psychology},
  number = {3}
}

@article{kleinManyLabsInvestigating2018,
  title = {Many Labs 2: {{Investigating}} Variation in Replicability across Samples and Settings},
  author = {Klein, Richard A. and Vianello, Michelangelo and Hasselman, Fred and Adams, Byron G. and Reginald B. Adams, Jr. and Alper, Sinan and Aveyard, Mark and Axt, Jordan R. and Babalola, Mayowa T. and Bahn{\'i}k, {\v S}t{\v e}p{\'a}n and Batra, Rishtee and Berkics, Mih{\'a}ly and Bernstein, Michael J. and Berry, Daniel R. and Bialobrzeska, Olga and Binan, Evans Dami and Bocian, Konrad and Brandt, Mark J. and Busching, Robert and R{\'e}dei, Anna Cabak and Cai, Huajian and Cambier, Fanny and Cantarero, Katarzyna and Carmichael, Cheryl L. and Ceric, Francisco and Chandler, Jesse and Chang, Jen-Ho and Chatard, Armand and Chen, Eva E. and Cheong, Winnee and Cicero, David C. and Coen, Sharon and Coleman, Jennifer A. and Collisson, Brian and Conway, Morgan A. and Corker, Katherine S. and Curran, Paul G. and Cushman, Fiery and Dagona, Zubairu K. and Dalgar, Ilker and Rosa, Anna Dalla and Davis, William E. and {de Bruijn}, Maaike and Schutter, Leander De and Devos, Thierry and {de Vries}, Marieke and Do{\u g}ulu, Canay and Dozo, Nerisa and Dukes, Kristin Nicole and Dunham, Yarrow and Durrheim, Kevin and Ebersole, Charles R. and Edlund, John E. and Eller, Anja and English, Alexander Scott and Finck, Carolyn and Frankowska, Natalia and Freyre, Miguel-{\'A}ngel and Friedman, Mike and Galliani, Elisa Maria and Gandi, Joshua C. and Ghoshal, Tanuka and Giessner, Steffen R. and Gill, Tripat and Gnambs, Timo and G{\'o}mez, {\'A}ngel and Gonz{\'a}lez, Roberto and Graham, Jesse and Grahe, Jon E. and Grahek, Ivan and Green, Eva G. T. and Hai, Kakul and Haigh, Matthew and Haines, Elizabeth L. and Hall, Michael P. and Heffernan, Marie E. and Hicks, Joshua A. and Houdek, Petr and Huntsinger, Jeffrey R. and Huynh, Ho Phi and IJzerman, Hans and Inbar, Yoel and {Innes-Ker}, \AA se H. and {Jim{\'e}nez-Leal}, William and John, Melissa-Sue and {Joy-Gaba}, Jennifer A. and Kamilo{\u g}lu, Roza G. and Kappes, Heather Barry and Karabati, Serdar and Karick, Haruna and Keller, Victor N. and Kende, Anna and Kervyn, Nicolas and Kne{\v z}evi{\'c}, Goran and Kovacs, Carrie and Krueger, Lacy E. and Kurapov, German and Kurtz, Jamie and Lakens, Dani{\"e}l and Lazarevi{\'c}, Ljiljana B. and Levitan, Carmel A. and Neil A. Lewis, Jr. and Lins, Samuel and Lipsey, Nikolette P. and Losee, Joy E. and Maassen, Esther and Maitner, Angela T. and Malingumu, Winfrida and Mallett, Robyn K. and Marotta, Satia A. and Me\dj edovi{\'c}, Janko and {Mena-Pacheco}, Fernando and Milfont, Taciano L. and Morris, Wendy L. and Murphy, Sean C. and Myachykov, Andriy and Neave, Nick and Neijenhuijs, Koen and Nelson, Anthony J. and Neto, F{\'e}lix and Nichols, Austin Lee and Ocampo, Aaron and O'Donnell, Susan L. and Oikawa, Haruka and Oikawa, Masanori and Ong, Elsie and Orosz, G{\'a}bor and Osowiecka, Malgorzata and Packard, Grant and {P{\'e}rez-S{\'a}nchez}, Rolando and Petrovi{\'c}, Boban and Pilati, Ronaldo and Pinter, Brad and Podesta, Lysandra and Pogge, Gabrielle and Pollmann, Monique M. H. and Rutchick, Abraham M. and Saavedra, Patricio and Saeri, Alexander K. and Salomon, Erika and Schmidt, Kathleen and Sch{\"o}nbrodt, Felix D. and Sekerdej, Maciej B. and Sirlop{\'u}, David and Skorinko, Jeanine L. M. and Smith, Michael A. and {Smith-Castro}, Vanessa and Smolders, Karin C. H. J. and Sobkow, Agata and Sowden, Walter and Spachtholz, Philipp and Srivastava, Manini and Steiner, Troy G. and Stouten, Jeroen and Street, Chris N. H. and Sundfelt, Oskar K. and Szeto, Stephanie and Szumowska, Ewa and Tang, Andrew C. W. and Tanzer, Norbert and Tear, Morgan J. and Theriault, Jordan and Thomae, Manuela and Torres, David and Traczyk, Jakub and Tybur, Joshua M. and Ujhelyi, Adrienn and {van Aert}, Robbie C. M. and {van Assen}, Marcel A. L. M. and {van der Hulst}, Marije and {van Lange}, Paul A. M. and {van 't Veer}, Anna Elisabeth and Echeverr{\'i}a, Alejandro V{\'a}squez- and Vaughn, Leigh Ann and V{\'a}zquez, Alexandra and Vega, Luis Diego and Verniers, Catherine and Verschoor, Mark and Voermans, Ingrid P. J. and Vranka, Marek A. and Welch, Cheryl and Wichman, Aaron L. and Williams, Lisa A. and Wood, Michael and Woodzicka, Julie A. and Wronska, Marta K. and Young, Liane and Zelenski, John M. and Zhijia, Zeng and Nosek, Brian A.},
  year = {2018},
  volume = {1},
  pages = {443--490},
  doi = {10.1177/2515245918810225},
  abstract = {We conducted preregistered replications of 28 classic and contemporary published findings, with protocols that were peer reviewed in advance, to examine variation in effect magnitudes across samples and settings. Each protocol was administered to approximately half of 125 samples that comprised 15,305 participants from 36 countries and territories. Using the conventional criterion of statistical significance (p \textexclamdown{} .05), we found that 15 (54\%) of the replications provided evidence of a statistically significant effect in the same direction as the original finding. With a strict significance criterion (p \textexclamdown{} .0001), 14 (50\%) of the replications still provided such evidence, a reflection of the extremely high-powered design. Seven (25\%) of the replications yielded effect sizes larger than the original ones, and 21 (75\%) yielded effect sizes smaller than the original ones. The median comparable Cohen's ds were 0.60 for the original findings and 0.15 for the replications. The effect sizes were small (\textexclamdown{} 0.20) in 16 of the replications (57\%), and 9 effects (32\%) were in the direction opposite the direction of the original effect. Across settings, the Q statistic indicated significant heterogeneity in 11 (39\%) of the replication effects, and most of those were among the findings with the largest overall effect sizes; only 1 effect that was near zero in the aggregate showed significant heterogeneity according to this measure. Only 1 effect had a tau value greater than .20, an indication of moderate heterogeneity. Eight others had tau values near or slightly above .10, an indication of slight heterogeneity. Moderation tests indicated that very little heterogeneity was attributable to the order in which the tasks were performed or whether the tasks were administered in lab versus online. Exploratory comparisons revealed little heterogeneity between Western, educated, industrialized, rich, and democratic (WEIRD) cultures and less WEIRD cultures (i.e., cultures with relatively high and low WEIRDness scores, respectively). Cumulatively, variability in the observed effect sizes was attributable more to the effect being studied than to the sample or setting in which it was studied.},
  journal = {Advances in Methods and Practices in Psychological Science},
  number = {4}
}

@book{kurkiewiczDocstringProvidesDocstring2017,
  title = {Docstring: {{Provides}} Docstring Capabilities to r Functions},
  author = {Kurkiewicz, Dason},
  year = {2017}
}

@article{lakensJustifyYourAlpha2018,
  title = {Justify Your Alpha},
  author = {Lakens, Daniel and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and {van Harmelen}, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and {de Oliveira}, Cilene Lino and {de Xivry}, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k{a}}tkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  volume = {2},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  abstract = {In response to recommendations to redefine statistical significance to P {$\leq$} 0.005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
  journal = {Nature Human Behaviour},
  number = {3}
}

@techreport{lakensValuePreregistrationPsychological2019,
  title = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}: {{A Conceptual Analysis}}},
  shorttitle = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}},
  author = {Lakens, Daniel},
  year = {2019},
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/jbh4w},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.},
  file = {/Users/claudio/MEGA/Zotero/Lakens_2019_The Value of Preregistration for Psychological Science.pdf},
  keywords = {Letto},
  language = {en},
  type = {Preprint}
}

@article{laneEstimatingEffectSize1978,
  title = {Estimating Effect Size: {{Bias}} Resulting from the Significance Criterion in Editorial Decisions},
  author = {Lane, David M and Dunlap, William P},
  year = {1978},
  volume = {31},
  pages = {107--112},
  publisher = {{Wiley Online Library}},
  doi = {10.1111/j.2044-8317.1978.tb00578.x},
  journal = {British Journal of Mathematical and Statistical Psychology},
  number = {2}
}

@article{luNoteTypeErrors2018,
  title = {A Note on {{Type S}}/{{M}} Errors in Hypothesis Testing},
  author = {Lu, Jiannan and Qiu, Yixuan and Deng, Alex},
  year = {2018},
  issn = {00071102},
  doi = {10.1111/bmsp.12132},
  file = {/Users/claudio/MEGA/Zotero/Lu et al_2018_A note on Type S-M errors in hypothesis testing.pdf},
  journal = {British Journal of Mathematical and Statistical Psychology},
  keywords = {Effect Size,Letto,Power Analysis,Type S and Type M errror},
  language = {en}
}

@book{mayoStatisticalInferenceSevere2018,
  title = {Statistical {{Inference}} as {{Severe Testing}}: {{How}} to {{Get Beyond}} the {{Statistics Wars}}},
  shorttitle = {Statistical {{Inference}} as {{Severe Testing}}},
  author = {Mayo, Deborah G.},
  year = {2018},
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781107286184},
  file = {/Users/claudio/MEGA/Zotero/Mayo_2018_Statistical Inference as Severe Testing.pdf},
  isbn = {978-1-107-28618-4 978-1-107-05413-4 978-1-107-66464-7},
  language = {en}
}

@article{opensciencecollaborationEstimatingReproducibilityPsychological2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  year = {2015},
  volume = {349},
  pages = {aac4716-aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  file = {/Users/claudio/MEGA/Zotero/Open Science Collaboration_2015_Estimating the reproducibility of psychological science.pdf},
  journal = {Science},
  language = {en},
  number = {6251}
}

@incollection{protzkoDeclineEffectsTypes2017,
  title = {Decline Effects: {{Types}}, Mechanisms, and Personal Reflections.},
  booktitle = {Psychological Science under Scrutiny: {{Recent}} Challenges and Proposed Solutions.},
  author = {Protzko, John and Schooler, Jonathan W.},
  year = {2017},
  pages = {85--107},
  publisher = {{Wiley-Blackwell}},
  doi = {10.1002/9781119095910.ch6},
  abstract = {In this chapter, we consider four general types of declining effect sizes, each of which relates to the hypothetical true effect size of the finding in question at the time it was originally reported. False positive decline effects occur when there actually was no true effect when the research was conducted, initially reported positive findings were instead a statistical or methodological artifact. Inflated decline effects occur when a true effect did exist but the initially reported studies artificially inflated the estimate of its size. Under-specified decline effects occur when a true effect originally existed but its necessary conditions were under-specified, as a result subsequent studies faded to include those conditions and thereby observed smaller effects. Finally, genuinely decreasing decline effects occur when the true effect size was originally and accurately reported but, for some reason, the true effect genuinely declines in magnitude over time. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  isbn = {978-1-118-66107-9 (Paperback); 978-1-118-66108-6 (PDF); 978-1-118-66104-8 (EPUB)},
  keywords = {*Effect Size (Statistical),*Methodology,Statistical Analysis}
}

@article{rohrerThinkingClearlyCorrelations2018,
  title = {Thinking Clearly about Correlations and Causation: {{Graphical}} Causal Models for Observational Data},
  author = {Rohrer, Julia M.},
  year = {2018},
  volume = {1},
  pages = {27--42},
  doi = {10.1177/2515245917745629},
  abstract = {Correlation does not imply causation; but often, observational data are the only option, even though the research question at hand involves causality. This article discusses causal inference based on observational data, introducing readers to graphical causal models that can provide a powerful tool for thinking more clearly about the interrelations between variables. Topics covered include the rationale behind the statistical control of third variables, common procedures for statistical control, and what can go wrong during their implementation. Certain types of third variables\textemdash colliders and mediators\textemdash should not be controlled for because that can actually move the estimate of an association away from the value of the causal effect of interest. More subtle variations of such harmful control include using unrepresentative samples, which can undermine the validity of causal conclusions, and statistically controlling for mediators. Drawing valid causal inferences on the basis of observational data is not a mechanistic procedure but rather always depends on assumptions that require domain knowledge and that can be more or less plausible. However, this caveat holds not only for research based on observational data, but for all empirical research endeavors.},
  eprint = {https://doi.org/10.1177/2515245917745629},
  journal = {Advances in Methods and Practices in Psychological Science},
  number = {1}
}

@article{schoolerTurningLensScience2014,
  title = {Turning the {{Lens}} of {{Science}} on {{Itself}}: {{Verbal Overshadowing}}, {{Replication}}, and {{Metascience}}},
  author = {Schooler, J.W.},
  year = {2014},
  volume = {9},
  pages = {579--584},
  doi = {10.1177/1745691614547878},
  journal = {Perspectives on Psychological Science},
  language = {English},
  number = {5}
}

@article{vasishthStatisticalSignificanceFilter2018,
  title = {The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability},
  author = {Vasishth, Shravan and Mertzen, Daniela and J{\"a}ger, Lena A. and Gelman, Andrew},
  year = {2018},
  volume = {103},
  pages = {151--175},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2018.07.004},
  abstract = {It is well-known in statistics (e.g., Gelman \& Carlin, 2014) that treating a result as publishable just because the p-value is less than 0.05 leads to overoptimistic expectations of replicability. These effects get published, leading to an overconfident belief in replicability. We demonstrate the adverse consequences of this statistical significance filter by conducting seven direct replication attempts (268 participants in total) of a recent paper (Levy \& Keller, 2013). We show that the published claims are so noisy that even non-significant results are fully compatible with them. We also demonstrate the contrast between such small-sample studies and a larger-sample study; the latter generally yields a less noisy estimate but also a smaller effect magnitude, which looks less compelling but is more realistic. We reiterate several suggestions from the methodology literature for improving current practices.},
  journal = {Journal of Memory and Language},
  keywords = {Bayesian data analysis,Expectation,Locality,Parameter estimation,Replicability,Surprisal,Type M error}
}

@book{venablesModernAppliedStatistics2002,
  ids = {venablesModernAppliedStatistics2002a},
  title = {Modern {{Applied Statistics}} with {{S}}},
  author = {Venables, W N and Ripley, B D},
  year = {2002},
  publisher = {{Springer}},
  file = {/Users/claudio/MEGA/Zotero/Venables_Ripley_2002_Modern Applied Statistics with S.pdf},
  language = {en}
}

@article{vulPuzzlinglyHighCorrelations2009,
  title = {Puzzlingly High Correlations in {{fMRI}} Studies of Emotion, Personality, and Social Cognition},
  author = {Vul, Edward and Harris, Christine and Winkielman, Piotr and Pashler, Harold},
  year = {2009},
  volume = {4},
  pages = {274--290},
  doi = {10.1111/j.1745-6924.2009.01125.x},
  abstract = {Functional magnetic resonance imaging (fMRI) studiesofemotion, personality, and social cognition have drawn much attention in recent years, with high-profile studies frequently reporting extremely high (e.g., \textquestiondown.8) correlations between brain activation and personality measures. We show that these correlations are higher than should be expected given the (evidently limited) reliability of both fMRI and personality measures. The high correlations are all the more puzzling because method sections rarely contain much detail about how the correlations were obtained. We surveyed authors of 55 articles that reported findings of this kind to determine a few details on how these correlations were computed. More than half acknowledged using a strategy that computes separate correlations for individual voxels and reports means of only those voxels exceeding chosen thresholds. We show how this nonindependent analysis inflates correlations while yielding reassuring-looking scattergrams. This analysis technique was used to obtain the vast majority of the implausibly high correlations in our survey sample. In addition, we argue that, in some cases, other analysis problems likely created entirely spurious correlations. We outline how the data from these studies could be reanalyzed with unbiased methods to provide accurate estimates of the correlations in question and urge authors to perform such reanalyses. The underlying problems described here appear to be common in fMRI research of many kinds\textemdash not just in studies of emotion, personality, and social cognition.},
  journal = {Perspectives on Psychological Science},
  number = {3}
}

@incollection{vulSuspiciouslyHighCorrelations2017,
  title = {Suspiciously High Correlations in Brain Imaging Research},
  booktitle = {Psychological Science under Scrutiny},
  author = {Vul, Edward and Pashler, Harold},
  year = {2017},
  pages = {196--220},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781119095910.ch11},
  abstract = {Summary Blood-oxygen-level dependent (BOLD) activity in a small region of the brain accounted for the great majority of the variance in speed with which subjects walk out of the experiment. While trying to estimate the maximum plausible population correlation between fMRI measures and social behavior based on psychometric considerations, it seemed that the upper bound should be around 0.75. A single massively multivariate analysis in a whole-brain, across-subject fMRI experiment is analogous to a whole field carrying out many experiments. Publication bias inflates the effect sizes in a given field by filtering many executed studies to get just those that passed a significance threshold. The challenge of whole-brain fMRI parallels the challenge faced by genetics and domains in which the candidate pool of variables exceeds the number of independent measurements. In non-independent whole-brain correlation studies, instead of reporting the average correlation of a detected cluster, the investigators instead report the ``peak voxel'' from that cluster.},
  chapter = {11},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781119095910.ch11},
  isbn = {978-1-119-09591-0},
  keywords = {blood-oxygen-level dependent activity,brain imaging research,fMRI,non-independent whole-brain correlation studies,population correlation,psychometric considerations,publication bias,social neuroscience}
}

@article{yarkoniBigCorrelationsLittle2009,
  title = {Big {{Correlations}} in {{Little Studies}}: {{Inflated fMRI Correlations Reflect Low Statistical Power}}\textemdash{{Commentary}} on {{Vul}} et al. (2009)},
  author = {Yarkoni, Tal},
  year = {2009},
  volume = {4},
  pages = {294--298},
  publisher = {{SAGE Publications Inc}},
  issn = {1745-6916},
  doi = {10.1111/j.1745-6924.2009.01127.x},
  abstract = {Vul, Harris, Winkielman, and Pashler (2009), (this issue) argue that correlations in many cognitive neuroscience studies are grossly inflated due to a widespread tendency to use nonindependent analyses. In this article, I argue that Vul et al.'s primary conclusion is correct, but for different reasons than they suggest. I demonstrate that the primary cause of grossly inflated correlations in whole-brain fMRI analyses is not nonindependence, but the pernicious combination of small sample sizes and stringent alpha-correction levels. Far from defusing Vul et al.'s conclusions, the simulations presented suggest that the level of inflation may be even worse than Vul et al.'s empirical analysis would suggest.},
  journal = {Perspectives on Psychological Science},
  number = {3}
}

@article{youngWhyCurrentPublication2008,
  title = {Why Current Publication Practices May Distort Science},
  author = {Young, Neal S and Ioannidis, John P. A and {Al-Ubaydli}, Omar},
  year = {2008},
  volume = {5},
  pages = {1--5},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pmed.0050201},
  abstract = {John Ioannidis and colleagues argue that the current system of publication in biomedical research provides a distorted view of the reality of scientific data.},
  journal = {PLOS Medicine},
  number = {10}
}


